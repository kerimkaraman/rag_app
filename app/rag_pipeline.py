from pymilvus import connections, Collection
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from ollama import chat  # LLaMA3â€™Ã¼ localden Ã§aÄŸÄ±racaÄŸÄ±z
import numpy as np

# 1ï¸âƒ£ Milvus baÄŸlantÄ±sÄ±
connections.connect("default", host="localhost", port="19530")
collection = Collection("my_collection")

# 2ï¸âƒ£ Embedding modeli
model = SentenceTransformer("all-MiniLM-L6-v2")

def query_milvus_with_cosine(user_query, top_k=2):
    """
    KullanÄ±cÄ±nÄ±n sorusunu embed edip Milvus'taki vektÃ¶rlerle cosine similarity'e gÃ¶re karÅŸÄ±laÅŸtÄ±rÄ±r.
    """
    # Queryâ€™yi embedle
    query_vec = model.encode([user_query])

    # Milvus'taki tÃ¼m embeddingâ€™leri ve metadataâ€™yÄ± al
    results = collection.query(
        expr="id >= 0",
        output_fields=["id", "text", "embedding"]
    )

    # Embeddingâ€™leri numpy arrayâ€™e dÃ¶nÃ¼ÅŸtÃ¼r
    vectors = np.array([r["embedding"] for r in results])
    texts = [r["text"] for r in results]

    # Cosine benzerliÄŸini hesapla
    sims = cosine_similarity(query_vec, vectors)[0]

    # En benzer top_k dokÃ¼manÄ± seÃ§
    top_indices = np.argsort(sims)[::-1][:top_k]
    top_texts = [texts[i] for i in top_indices]

    return top_texts


def rag_pipeline(user_query):
    """
    RAG zinciri:
    1ï¸âƒ£ Milvus'tan ilgili dokÃ¼manlarÄ± alÄ±r
    2ï¸âƒ£ DokÃ¼manlarÄ± birleÅŸtirip LLaMA3'e baÄŸlam olarak yollar
    3ï¸âƒ£ Nihai cevabÄ± Ã¼retir
    """
    context_docs = query_milvus_with_cosine(user_query)

    context = "\n".join(context_docs)

    prompt = f"""
You are a helpful AI assistant with access to retrieved context documents.
Use the information below to answer the user's question accurately.

Context:
{context}

User Question: {user_query}

Answer:
"""

    # LLaMA3 ile yanÄ±t Ã¼retimi (local)
    response = chat(model="llama3", messages=[{"role": "user", "content": prompt}])

    return response["message"]["content"]


if __name__ == "__main__":
    query = input("Ask something: ")
    answer = rag_pipeline(query)
    print("\nğŸ§  LLaMA3 Response:\n", answer)
